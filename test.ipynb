{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Kprerak-iisc/data-science/blob/b8b8e402e4a0ba7aa9c946f56c20e1d0251c158b/NLP/QA/BiDAF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re, os, string, typing, gc, json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    '''\n",
    "    Loads the JSON file of the Squad dataset.\n",
    "    Returns the json object of the dataset.\n",
    "    '''\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    print(\"Length of data: \", len(data['data']))\n",
    "    print(\"Data Keys: \", data['data'][0].keys())\n",
    "    print(\"Title: \", data['data'][0]['title'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data:dict) -> list:\n",
    "    \"\"\"\n",
    "    Squad 데이터 세트의 JSON 파일을\n",
    "    key와 values을 입력하고 dictionry를 반환합니다.\n",
    "    컨텍스트, query 및 레이블 트리플렛이 각 사전의 키가됩니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data['data']\n",
    "    qa_list = []\n",
    "\n",
    "    for paragraphs in data:\n",
    "\n",
    "        for para in paragraphs['paragraphs']:\n",
    "            context = para['context']\n",
    "\n",
    "            for qa in para['qas']:\n",
    "                \n",
    "                id = qa['id']\n",
    "                question = qa['question']\n",
    "                \n",
    "                for ans in qa['answers']:\n",
    "                    answer = ans['text']\n",
    "                    ans_start = ans['answer_start']\n",
    "                    ans_end = ans_start + len(answer)\n",
    "                    \n",
    "                    qa_dict = {}\n",
    "                    qa_dict['id'] = id\n",
    "                    qa_dict['context'] = context\n",
    "                    qa_dict['question'] = question\n",
    "                    qa_dict['label'] = [ans_start, ans_end]\n",
    "\n",
    "                    qa_dict['answer'] = answer\n",
    "                    qa_list.append(qa_dict)    \n",
    "\n",
    "    \n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_large_examples(df):\n",
    "    \"\"\"\n",
    "    임의의 값 이상의 컨텍스트 길이, 쿼리 길이 및 answer 길이가있는 예제의 ID를 반환합니다.\n",
    "    그런 다음 이러한 ID를 데이터 프레임에서 삭제할 수 있습니다.\n",
    "    이것은 QANet에서 명시 적으로 언급되지만 다른 모델에서도 가능합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    ctx_lens = []\n",
    "    query_lens = []\n",
    "    ans_lens = []\n",
    "    for index, row in df.iterrows():\n",
    "        cts_tokens = [w.text for w in nlp(row.cntext, disable=['parser', 'ner', 'tagger'])]\n",
    "        if len(ctx_tokens) > 400:\n",
    "            ctx_lens.append(row.name)\n",
    "            \n",
    "        query_tokens = [w.text for w in nlp(row.question, disable=['parser', 'tagger', 'ner'])]\n",
    "        if len(query_tokens) > 50:\n",
    "            query_lens.append(row.name)\n",
    "            \n",
    "        ans_tokens = [w.text for w in nlp(row.answer, disable=['parser', 'tagger', 'ner'])]\n",
    "        if len(ans_tokens) > 30:\n",
    "            ans_lens.append(row.name)\n",
    "            \n",
    "        assert row.name == index\n",
    "\n",
    "    return set(ans_lens + ctx_lens + query_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_text_for_vocab(dfs:list):\n",
    "    \"\"\"\n",
    "    문맥과 질문에서 텍스트를 수집하여 어휘를 만듭니다.\n",
    "    \n",
    "    : param dfs : SQUAD 데이터 세트의 데이터 프레임 목록.\n",
    "    : returns : 컨텍스트 및 질문 목록\n",
    "    \"\"\"\n",
    "    \n",
    "    text = []\n",
    "    total = 0\n",
    "    for df in dfs:\n",
    "        unique_contexts = list(df.context.unique())\n",
    "        unique_questions = list(df.question.unique())\n",
    "        total += df.context.nunique() + df.question.nunique()\n",
    "        text.extend(unique_contexts + unique_questions)\n",
    "        \n",
    "    assert len(text) == total\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vocab(vocab_text):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에서 word-level의 어휘를 만듭니다.\n",
    "    \n",
    "    : param list vocab_text : 컨텍스트 및 질문 목록\n",
    "    : return\n",
    "        dict word2idx : 단어를 인덱스로 매핑하는 단어\n",
    "        dict idx2word : 정수 대 단어 매핑\n",
    "        list word_vocab : 빈도별로 정렬 된 단어 목록\n",
    "    \"\"\"\n",
    "    \n",
    "    words = []\n",
    "    for sent in vocab_text:\n",
    "        for word in nlp(sent, disable=['parser', 'tagger', 'ner']):\n",
    "            words.append(word.text)\n",
    "            \n",
    "    word_counter = Counter(words)\n",
    "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "    print(f\"raw-vocab: {len(word_vocab)}\")\n",
    "    print(f\"glove-vocab: {len(word_vocab)}\")\n",
    "    word_vocab.insert(0, '<unk>')\n",
    "    word_vocab.insert(1, '<pad>')\n",
    "    print(f\"vocab-length: {len(word_vocab)}\")\n",
    "    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\n",
    "    print(f\"word2idx-length: {len(word2idx)}\")\n",
    "    idx2word = {v:k for k,v in word2idx.items()}\n",
    "    \n",
    "    \n",
    "    return word2idx, idx2word, word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_vocab(vocab_text):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에서 char-level의 어휘를 만듭니다.\n",
    "    \n",
    "    : param list vocab_text : 컨텍스트 및 질문 목록\n",
    "    : return\n",
    "        dict char2idx : 문자와 단어의 인덱스 매핑\n",
    "        list char_vocab : 빈도별로 정렬 된 문자 목록\n",
    "    \"\"\"\n",
    "    \n",
    "    chars = []\n",
    "    for sent in vocab_text:\n",
    "        for ch in sent:\n",
    "            chars.append(ch)\n",
    "            \n",
    "    char_counter = Counter(chars)\n",
    "    char_vocab = sorted(char_counter, key=char_counter.get, reverse=True)\n",
    "    print(f'raw-char-vocab: {len(char_vocab)}')\n",
    "    high_freq_char = [char for char, count in char_counter.items() if count>=20]\n",
    "    char_vocab = list(set(char_vocab).intersection(set(high_freq_char)))\n",
    "    print(f'char-vocab-intersect: {len(char_vocab)}')\n",
    "    char_vocab.insert(0, '<unk>')\n",
    "    char_vocab.insert(1, '<pad>')\n",
    "    char2idx = {char:idx for idx, char in enumerate(char_vocab)}\n",
    "    print(f'char2idx-length: {len(char2idx)}')\n",
    "    \n",
    "    return char2idx, char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_ids(text, word2idx):\n",
    "    \"\"\"\n",
    "    각 단어를 매핑하여 컨텍스트 텍스트를 해당 ID로 변환합니다.\n",
    "    word2idx 사용. 입력 텍스트는 먼저 spacy 토크 나이저를 사용하여 토큰 화됩니다.\n",
    "    \n",
    "    : param str text : 변환 할 컨텍스트 텍스트\n",
    "    : return list context_ids : 매핑 된 ID 목록\n",
    "    \n",
    "    : raises assertion error : 상태 체크\n",
    "    \"\"\"\n",
    "    \n",
    "    context_tokens = [w.text for w in nlp(text, disable=['parser', 'tagger', 'ner'])]\n",
    "    context_ids = [word2idx[word] for word in context_tokens]\n",
    "    \n",
    "    assert len(context_ids) == len(context_tokens)\n",
    "    return context_ids\n",
    "\n",
    "def question_to_ids(text, word2idx):\n",
    "    \"\"\"\n",
    "    각 단어를 매핑하여 질문 텍스트를 해당 ID로 변환합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    question_tokens = [w.text for w in nlp(text, disable=['parser', 'tagger', 'ner'])]\n",
    "    question_ids = [word2idx[word] for word in question_tokens]\n",
    "    \n",
    "    assert len(question_ids) == len(question_tokens)\n",
    "    return question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_indices(df, idx2word):\n",
    "    \"\"\"\n",
    "    위에서 언급 한 테스트를 수행합니다. \n",
    "    이 방법은 각 예제의 context_ids와 관련된 답변의 시작과 끝을 가져옵니다.\n",
    "    \n",
    "    : param 데이터 프레임 df : SQUAD df\n",
    "    : returns\n",
    "        list start_value_error : example idx 텍스트에서 시작 idx를 찾을 수 없을 때 \n",
    "        list end_value_error : example idx 텍스트에서 종료 idx를 찾을 수 없을 때 \n",
    "        list assert_error : 어설 션 오류에 실패한 예. 대다수는 위의 오류로 인한 것입니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_value_error = []\n",
    "    end_value_error = []\n",
    "    assert_error = []\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        answer_tokens = [w.text for w in nlp(row['answer'], disable=['parser',' tagger', 'ner'])]\n",
    "        start_token = answer_tokens[0]\n",
    "        end_token = answer_tokens[-1]\n",
    "        \n",
    "        context_span = [(word.idx, word.idx + len(word.text))\n",
    "                       for word in nlp(row['context'], disable=['parser', 'tagger', 'ner'])]\n",
    "        \n",
    "        starts, ends = zip(*context_span)\n",
    "        \n",
    "        answer_start, answer_end = row['label']\n",
    "        \n",
    "        try:\n",
    "            start_idx = starts.index(answer_start)\n",
    "        except:\n",
    "            start_value_error.append(index)\n",
    "        \n",
    "        try:\n",
    "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n",
    "            assert idx2word[row['context_ids'][end_idx]] == answer_tokens[-1]\n",
    "        except:\n",
    "            assert_error.append(index)\n",
    "            \n",
    "    return start_value_error, end_value_error, assert_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_indices(df, idx2word):\n",
    "    \"\"\"\n",
    "    test_indices함수에서 오류를 가져와 에러 반환\n",
    "    \"\"\"\n",
    "    \n",
    "    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\n",
    "    err_idx = start_value_error + end_value_error + assert_error\n",
    "    err_idx = set(err_idx)\n",
    "    print(f'Error indices: {len(err_idx)}')\n",
    "    \n",
    "    return err_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_answer(row, idx2word):\n",
    "    \"\"\"\n",
    "    데이터 프레임의 행 또는 하나의 training example를 가져와 계산하여\n",
    "    answer의 시작 및 끝 위치의 튜플을 반환합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    context_span = [(word.idx, word.idx + len(word.text)) \n",
    "                    for word in nlp(row.context, disable=['parser','tagger','ner'])]\n",
    "    starts, ends = zip(*context_span)\n",
    "    \n",
    "    answer_start, answer_end = row.label\n",
    "    start_idx = starts.index(answer_start)\n",
    " \n",
    "    end_idx  = ends.index(answer_end)\n",
    "    \n",
    "    ans_toks = [w.text for w in nlp(row.answer,disable=['parser','tagger','ner'])]\n",
    "    ans_start = ans_toks[0]\n",
    "    ans_end = ans_toks[-1]\n",
    "    assert idx2word[row.context_ids[start_idx]] == ans_start\n",
    "    assert idx2word[row.context_ids[end_idx]] == ans_end\n",
    "    \n",
    "    return [start_idx, end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import torch\n",
    "from torch import nn\n",
    "import json, re, unicodedata, string, typing, time\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "nlp = spacy.load('en')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Beyoncé\n",
      "Length of data:  35\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Normans\n",
      "Train list len :  86821\n",
      "Valid list lne :  20302\n"
     ]
    }
   ],
   "source": [
    "train_data = load_json(r'C:\\Users\\admin\\jupyter\\pytorch\\Bert\\squad_train.json')\n",
    "valid_data = load_json(r'C:\\Users\\admin\\jupyter\\pytorch\\Bert\\squad_dev.json')\n",
    "\n",
    "train_list = parse_data(train_data)\n",
    "valid_list = parse_data(valid_data)\n",
    "\n",
    "print('Train list len : ', len(train_list))\n",
    "print('Valid list lne : ', len(valid_list))\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "valid_df = pd.DataFrame(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spaces(text):\n",
    "    \"\"\"\n",
    "    정규화 과정\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s', ' ',text)\n",
    "    return text\n",
    "\n",
    "train_df.context = train_df.context.apply(normalize_spaces)\n",
    "valid_df.context = valid_df.context.apply(normalize_spaces)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>[269, 286]</td>\n",
       "      <td>in the late 1990s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>[207, 226]</td>\n",
       "      <td>singing and dancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>[526, 530]</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>[166, 180]</td>\n",
       "      <td>Houston, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>[276, 286]</td>\n",
       "      <td>late 1990s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "1  56be85543aeaaa14008c9065   \n",
       "2  56be85543aeaaa14008c9066   \n",
       "3  56bf6b0f3aeaaa14008c9601   \n",
       "4  56bf6b0f3aeaaa14008c9602   \n",
       "\n",
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0           When did Beyonce start becoming popular?  [269, 286]   \n",
       "1  What areas did Beyonce compete in when she was...  [207, 226]   \n",
       "2  When did Beyonce leave Destiny's Child and bec...  [526, 530]   \n",
       "3      In what city and state did Beyonce  grow up?   [166, 180]   \n",
       "4         In which decade did Beyonce become famous?  [276, 286]   \n",
       "\n",
       "                answer  \n",
       "0    in the late 1990s  \n",
       "1  singing and dancing  \n",
       "2                 2003  \n",
       "3       Houston, Texas  \n",
       "4           late 1990s  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 371 ms\n",
      "Number of sentences in dataset:  112776\n"
     ]
    }
   ],
   "source": [
    "%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
    "print(\"Number of sentences in dataset: \", len(vocab_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 108286\n",
      "glove-vocab: 108286\n",
      "vocab-length: 108288\n",
      "word2idx-length: 108288\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n",
      "Wall time: 17.3 s\n",
      "Wall time: 6.49 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%time train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "%time valid_df['context_ids'] = valid_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "\n",
    "%time train_df['question_ids'] = train_df.question.apply(question_to_ids, word2idx = word2idx)\n",
    "%time valid_df['question_ids'] = valid_df.question.apply(question_to_ids, word2idx = word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error indices: 86821\n",
      "Error indices: 20302\n"
     ]
    }
   ],
   "source": [
    "train_err = get_error_indices(train_df, idx2word)\n",
    "valid_err = get_error_indices(valid_df, idx2word)\n",
    "\n",
    "train_df.drop(train_err, inplace=True)\n",
    "valid_df.drop(valid_err, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 7, placement implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label_idx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, item, value)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label_idx'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-41bba45da2e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvalid_label_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_answer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_idx'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_label_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_idx'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_label_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2938\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3000\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3001\u001b[1;33m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3003\u001b[0m         \u001b[1;31m# check if we are modifying a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3623\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3624\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3625\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset\u001b[1;34m(self, item, value)\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;31m# This item wasn't present, just insert at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1181\u001b[1;33m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[0;32m   3051\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3053\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m   2599\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2601\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2603\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    125\u001b[0m                 \u001b[1;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[1;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 7, placement implies 1"
     ]
    }
   ],
   "source": [
    "# 시작과 끝 위치 반환\n",
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "\n",
    "train_df['label_idx'] = train_label_idx\n",
    "valid_df['label_idx'] = valid_label_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
