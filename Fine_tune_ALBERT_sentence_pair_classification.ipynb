{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb#section04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\abc\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: dill in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (1.18.5)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (4.47.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (2.24.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\abc\\anaconda3\\lib\\site-packages (from datasets) (1.0.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.25.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abc\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset, load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\abc\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\abc\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-b17e521514dba5f3.arrow and C:\\Users\\abc\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4\\cache-1eed49699dffac08.arrow\n"
     ]
    }
   ],
   "source": [
    "split = dataset['train'].train_test_split(test_size=0.1, seed=1)\n",
    "train = split['train']\n",
    "val = split['test']\n",
    "test = dataset['validation']\n",
    "\n",
    "df_train = pd.DataFrame(train)\n",
    "df_val = pd.DataFrame(val)\n",
    "df_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3301, 4)\n",
      "(367, 4)\n",
      "(408, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3349</td>\n",
       "      <td>1</td>\n",
       "      <td>The American troops , who also defend the mayo...</td>\n",
       "      <td>The American troops from 3-15 infantry , who a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1806</td>\n",
       "      <td>0</td>\n",
       "      <td>Last week , his lawyers asked Warner to grant ...</td>\n",
       "      <td>Last week , his lawyers asked Gov. Mark R. War...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2681</td>\n",
       "      <td>1</td>\n",
       "      <td>Their election increases the board from seven ...</td>\n",
       "      <td>Their appointments increase Berkshire 's board...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>Dolores Mahoy , 68 , of Colorado Springs , Col...</td>\n",
       "      <td>Dolores E. Mahoy , 68 , of Colorado Springs is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2993</td>\n",
       "      <td>1</td>\n",
       "      <td>\" They were an inspirational couple , selfless...</td>\n",
       "      <td>He said : “ They were an inspirational couple ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx  label                                          sentence1  \\\n",
       "0  3349      1  The American troops , who also defend the mayo...   \n",
       "1  1806      0  Last week , his lawyers asked Warner to grant ...   \n",
       "2  2681      1  Their election increases the board from seven ...   \n",
       "3  2001      1  Dolores Mahoy , 68 , of Colorado Springs , Col...   \n",
       "4  2993      1  \" They were an inspirational couple , selfless...   \n",
       "\n",
       "                                           sentence2  \n",
       "0  The American troops from 3-15 infantry , who a...  \n",
       "1  Last week , his lawyers asked Gov. Mark R. War...  \n",
       "2  Their appointments increase Berkshire 's board...  \n",
       "3  Dolores E. Mahoy , 68 , of Colorado Springs is...  \n",
       "4  He said : “ They were an inspirational couple ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n",
    "        self.data = data\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.with_labels = with_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sent1 = str(self.data.loc[index, 'sentence1'])\n",
    "        sent2 = str(self.data.loc[index, 'sentence2'])\n",
    "        \n",
    "        encoded_pair = self.tokenizer(sent1, sent2, padding='max_length', truncation=True, max_length=self.maxlen, return_tensors='pt')\n",
    "        token_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attn_masks = encodedd_pair['attention_mask'].squeeze(0)\n",
    "        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)\n",
    "        \n",
    "        if self.with_labels:\n",
    "            label = self.data.loc[index, 'label']\n",
    "            return token_ids, attn_masks, token_type_ids, label\n",
    "        else:\n",
    "            return token_ids, attn_masks, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n",
    "        super(SentencePairClassifier, self).__init__()\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model)\n",
    "        \n",
    "        if bert_model == \"albert-base-v2\":  # 12M parameters\n",
    "            hidden_size = 768\n",
    "        elif bert_model == \"albert-large-v2\":  # 18M parameters\n",
    "            hidden_size = 1024\n",
    "        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n",
    "            hidden_size = 2048\n",
    "        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n",
    "            hidden_size = 4096\n",
    "        elif bert_model == \"bert-base-uncased\": # 110M parameters\n",
    "            hidden_size = 768\n",
    "\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.cls_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, input_ids, attn_masks, token_type_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -input_ids : Tensor  containing token ids\n",
    "            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n",
    "            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n",
    "        '''\n",
    "\n",
    "        cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n",
    "\n",
    "        logits = self.cls_layer(self.dropout(pooler_output))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHEED'] = str(seed)\n",
    "    \n",
    "def evaluate_loss(net, device, criterion, dataloader):\n",
    "    net.eval()\n",
    "    \n",
    "    mean_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n",
    "            seq, attn_masks, token_type_ids, labels = seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            logits = net(seq, attn_masks, token_type_ids)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            count += 1\n",
    "            \n",
    "    return mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n",
    "\n",
    "    best_loss = np.Inf\n",
    "    best_ep = 1\n",
    "    nb_iterations = len(train_loader)\n",
    "    print_every = nb_iterations // 5  # print the training loss 5 times per epoch\n",
    "    iters = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            # Converting to cuda tensors\n",
    "            seq, attn_masks, token_type_ids, labels = \\\n",
    "                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n",
    "    \n",
    "            # Enables autocasting for the forward pass (model + loss)\n",
    "            with autocast():\n",
    "                # Obtaining the logits from the model\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "\n",
    "                # Computing loss\n",
    "                loss = criterion(logits.squeeze(-1), labels.float())\n",
    "                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n",
    "\n",
    "            # Backpropagating the gradients\n",
    "            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (it + 1) % iters_to_accumulate == 0:\n",
    "                # Optimization step\n",
    "                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n",
    "                # otherwise, opti.step() is skipped.\n",
    "                scaler.step(opti)\n",
    "                # Updates the scale for next iteration.\n",
    "                scaler.update()\n",
    "                # Adjust the learning rate based on the number of iterations.\n",
    "                lr_scheduler.step()\n",
    "                # Clear gradients\n",
    "                opti.zero_grad()\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (it + 1) % print_every == 0:  # Print training loss information\n",
    "                print()\n",
    "                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n",
    "                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "\n",
    "        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n",
    "        print()\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n",
    "            print()\n",
    "            net_copy = copy.deepcopy(net)  # save a copy of the model\n",
    "            best_loss = val_loss\n",
    "            best_ep = ep + 1\n",
    "\n",
    "    # Saving the model\n",
    "    path_to_model='models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n",
    "    torch.save(net_copy.state_dict(), path_to_model)\n",
    "    print(\"The model has been saved in {}\".format(path_to_model))\n",
    "\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"albert-base-v2\"  \n",
    "freeze_bert = False  \n",
    "maxlen = 128  \n",
    "bs = 16  \n",
    "iters_to_accumulate = 2  \n",
    "lr = 2e-5  \n",
    "epochs = 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data...\n",
      "Reading validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/207 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#  Set all seeds to make reproducible results\n",
    "set_seed(1)\n",
    "\n",
    "# Creating instances of training and validation set\n",
    "print(\"Reading training data...\")\n",
    "train_set = CustomDataset(df_train, maxlen, bert_model)\n",
    "print(\"Reading validation data...\")\n",
    "val_set = CustomDataset(df_val, maxlen, bert_model)\n",
    "# Creating instances of training and validation dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=bs, num_workers=5)\n",
    "val_loader = DataLoader(val_set, batch_size=bs, num_workers=5)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n",
    "\n",
    "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\n",
    "num_warmup_steps = 0 # The number of steps for the warmup phase.\n",
    "num_training_steps = epochs * len(train_loader)  # The total number of training steps\n",
    "t_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs_from_logits(logits):\n",
    "    \"\"\"\n",
    "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    return probs.detach().cpu().numpy()\n",
    "\n",
    "def test_prediction(net, device, dataloader, with_labels=True, result_file=\"output.txt\"):\n",
    "    \"\"\"\n",
    "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    w = open(result_file, 'w')\n",
    "    probs_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if with_labels:\n",
    "            for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
    "                probs_all += probs.tolist()\n",
    "        else:\n",
    "            for seq, attn_masks, token_type_ids in tqdm(dataloader):\n",
    "                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n",
    "                logits = net(seq, attn_masks, token_type_ids)\n",
    "                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
    "                probs_all += probs.tolist()\n",
    "\n",
    "    w.writelines(str(prob)+'\\n' for prob in probs_all)\n",
    "    w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = '/content/models/albert-base-v2_lr_2e-05_val_loss_0.35007_ep_3.pt'  \n",
    "# path_to_model = '/content/models/...'  # You can add here your trained model\n",
    "\n",
    "path_to_output_file = 'results/output.txt'\n",
    "\n",
    "print(\"Reading test data...\")\n",
    "test_set = CustomDataset(df_test, maxlen, bert_model)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
    "\n",
    "model = SentencePairClassifier(bert_model)\n",
    "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "print()\n",
    "print(\"Loading the weights of the model...\")\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.to(device)\n",
    "\n",
    "print(\"Predicting on test data...\")\n",
    "test_prediction(net=model, device=device, dataloader=test_loader, with_labels=True,  # set the with_labels parameter to False if your want to get predictions on a dataset without labels\n",
    "                result_file=path_to_output_file)\n",
    "print()\n",
    "print(\"Predictions are available in : {}\".format(path_to_output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_output_file = 'results/output.txt'  # path to the file with prediction probabilities\n",
    "\n",
    "labels_test = df_test['label']  # true labels\n",
    "\n",
    "probs_test = pd.read_csv(path_to_output_file, header=None)[0]  # prediction probabilities\n",
    "threshold = 0.5   # you can adjust this threshold for your own dataset\n",
    "preds_test=(probs_test>=threshold).astype('uint8') # predicted labels using the above fixed threshold\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric._compute(predictions=preds_test, references=labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
