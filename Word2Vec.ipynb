{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0] * (len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\", word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 희소표현\n",
    "- 표현하고자 하는 단어의 인덱스만 1이고 나머지는 0인 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = torch.FloatTensor([1, 0, 0, 0, 0])\n",
    "cat = torch.FloatTensor([0, 1, 0, 0, 0])\n",
    "computer = torch.FloatTensor([0, 0, 1, 0, 0])\n",
    "netbook = torch.FloatTensor([0, 0, 0, 1, 0])\n",
    "book = torch.FloatTensor([0, 0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity : 코사인 유사성을 반환()\n",
    "print(torch.cosine_similarity(dog, cat, dim=0))\n",
    "print(torch.cosine_similarity(cat, computer, dim=0))\n",
    "print(torch.cosine_similarity(computer, netbook, dim=0))\n",
    "print(torch.cosine_similarity(netbook, book, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 밀집표현\n",
    "- 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
    "- 실수값을 가지게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드 임베딩\n",
    "- 단어를 밀집 벡터의 형태로 표현하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드 투 벡터(Word2Vec)\n",
    "- 단어 간 유사도를 반영\n",
    "### 분산 표현\n",
    "- 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다.\n",
    "\n",
    "## 종류\n",
    "\n",
    "### CBOW(Continuous Bag of Words)\n",
    "   - 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법\n",
    "   \n",
    "### Skip-Gram\n",
    "   - 중간에 있는 단어로 주변 단어들을 예측하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree # xml파일을 다루기 위한 라이브러리\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x1a43cb2da60>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "\n",
    "target_text = etree.parse(targetXML)\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 괄호제거 정규표현식 : r'\\([^)]*\\)'\n",
    "# re.sub(정규표현식, 치환할 문자, 치환될 대상)\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "    \n",
    "result = []\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 갯수 : 273424\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 갯수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim : Word2Vec를 지원하는 패키지\n",
    "- 손쉽게 단어를 임베딩 벡터로 변환가능\n",
    "\n",
    "Word2Vec \n",
    "- size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "- window = 컨텍스트 윈도우 크기\n",
    "- min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "- workers = 학습을 위한 프로세스 수\n",
    "- sg = 0은 CBOW, 1은 Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=result, size = 100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8489661812782288), ('guy', 0.8201931715011597), ('boy', 0.7743191719055176), ('lady', 0.7660802602767944), ('girl', 0.7516182661056519), ('soldier', 0.7428873777389526), ('gentleman', 0.7136614322662354), ('kid', 0.6987431049346924), ('poet', 0.6751818656921387), ('philosopher', 0.651231050491333)]\n"
     ]
    }
   ],
   "source": [
    "# model.wv.most_similar : 가장 유사한 단어 출력\n",
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec 모델 저장하고 로드하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('./eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8489661812782288), ('guy', 0.8201931715011597), ('boy', 0.7743191719055176), ('lady', 0.7660802602767944), ('girl', 0.7516182661056519), ('soldier', 0.7428873777389526), ('gentleman', 0.7136614322662354), ('kid', 0.6987431049346924), ('poet', 0.6751818656921387), ('philosopher', 0.651231050491333)]\n"
     ]
    }
   ],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'C:\\Users\\admin\\wiki_data.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 줄 : <doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\n",
      "\n",
      "2번째 줄 : 지미 카터\n",
      "\n",
      "3번째 줄 : 제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
      "\n",
      "4번째 줄 : 지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\n",
      "\n",
      "5번째 줄 : 1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if line != '\\n':\n",
    "        i = i + 1\n",
    "        print(\"%d번째 줄 : \"%i + line)\n",
    "    if i == 5:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) morphs : 형태소 추출\\\n",
    "2) pos : 품사 태깅(Part-of-speech tagging)\\\n",
    "3) nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000번째 while문.\n",
      "10000번째 while문.\n",
      "15000번째 while문.\n",
      "20000번째 while문.\n",
      "25000번째 while문.\n",
      "30000번째 while문.\n",
      "35000번째 while문.\n",
      "40000번째 while문.\n",
      "45000번째 while문.\n",
      "50000번째 while문.\n",
      "55000번째 while문.\n",
      "60000번째 while문.\n",
      "65000번째 while문.\n",
      "70000번째 while문.\n",
      "75000번째 while문.\n",
      "80000번째 while문.\n",
      "85000번째 while문.\n",
      "90000번째 while문.\n",
      "95000번째 while문.\n",
      "100000번째 while문.\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "fread = open(r'C:\\Users\\admin\\wiki_data.txt', encoding=\"utf8\")\n",
    "n = 0\n",
    "result = []\n",
    "\n",
    "while True:\n",
    "    line = fread.readline()\n",
    "    if n == 100000:\n",
    "        break\n",
    "    n = n + 1\n",
    "    if n % 5000 == 0:\n",
    "        print(\"%d번째 while문.\" %n)\n",
    "    #  norm은 문장을 정규화(이미지에서 0~1.0사이값 나오도록). stem은 각 단어에서 어간을 추출\n",
    "    tokenlist = okt.pos(line, stem=True, norm=True)\n",
    "    temp = []\n",
    "    for word in tokenlist:\n",
    "        if word[1] in [\"Noun\"]:\n",
    "            temp.append((word[0]))\n",
    "            \n",
    "    if temp:\n",
    "        result.append(temp)\n",
    "fread.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 40283\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('조선민주주의인민공화국', 0.8123728632926941), ('북한', 0.8041773438453674), ('수교', 0.7786298990249634), ('중화인민공화국', 0.7784789800643921), ('중화민국', 0.7668930292129517), ('독도', 0.762294352054596), ('자국', 0.76041179895401), ('개국', 0.7549357414245605), ('유엔', 0.7438442707061768), ('국기', 0.7422870397567749)]\n"
     ]
    }
   ],
   "source": [
    "model_result1 = model.wv.most_similar(\"대한민국\")\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
